+æsêÄÄÄÄa:doêÄÄÄÄ2.3-ÃÄÄÄÄes des approches traditionnelles
Les algorithmes de compression traditionnels utilis√©s dans Hadoop sont appliqu√©s de mani√®re statique, c'est-√†-dire que le m√™me algorithme et les m√™mes param√®tres de compression sont utilis√©s pour tous les types de fichiers, quelles que soient leurs caract√©ristiques. Cette approche pr√©sente plusieurs limitations :
Manque de flexibilit√© : Les algorithmes statiques ne tiennent pas compte de la variabilit√© des donn√©es. Par exemple, des fichiers textes volumineux et des fichiers binaires n√©cessitent des approches de compression diff√©rentes pour obtenir des performances optimales.
Perte d'efficacit√© : En raison de la diversit√© des types de donn√©es dans un environnement Hadoop, un algorithme de compression unique ne sera jamais optimal pour toutes les situations. Les fichiers avec une grande redondance b√©n√©ficieront davantage d'un algorithme √† fort taux de compression, tandis que les fichiers n√©cessitant un acc√®s rapide profiteront d'algorithmes plus rapides comme Snappy ou LZO.
Utilisation inefficace des ressources : La compression statique ne tient pas compte des ressources disponibles dans le cluster Hadoop. Par exemple, l'utilisation d'un algorithme lent comme Bzip2 dans un sc√©nario o√π la rapidit√© est essentielle peut entra√Æner une surcharge inutile du syst√®me et allonger les temps de traitement.
Absence d'adaptation dynamique : Les approches traditionnelles ne s'adaptent pas en temps r√©el aux changements dans les charges de travail, les mod√®les d'acc√®s aux donn√©es, ou les ressources disponibles (comme le CPU et la m√©moire). Cela peut entra√Æner une d√©gradation des performances lorsque les conditions changent.

3- Approche Propos√©e
            3.1-Architechture g√©n√©rale
L'architecture propos√©e repose sur un syst√®me multi-agents (SMA) int√©gr√© dans un cluster Hadoop. Ce syst√®me est con√ßu pour surveiller de mani√®re dynamique les donn√©es, s√©lectionner les algorithmes de compression appropri√©s, et ajuster les param√®tres de ces algorithmes en fonction des caract√©ristiques des donn√©es et des exigences de performance. L‚Äôobjectif principal est de rendre les processus de compression plus intelligents et adaptatifs.
Composants de l'architecture
Agents autonomes : Chaque agent a un r√¥le sp√©cifique et communique avec les autres agents ainsi qu'avec le syst√®me Hadoop. Ces agents fonctionnent de mani√®re asynchrone et coop√©rative, ce qui permet de traiter les donn√©es en temps r√©el ou quasi temps r√©el.
HDFS et MapReduce : Les agents interagissent avec le syst√®me de fichiers distribu√© HDFS et le moteur de traitement MapReduce de Hadoop. Ils surveillent les fichiers et ajustent la compression √† diff√©rents niveaux, notamment lors du stockage initial, des transferts de donn√©es interm√©diaires (shuffle), et des sorties finales.
Surveillance des donn√©es : Les agents recueillent des informations sur les fichiers (taille, type, fr√©quence d'acc√®s) et sur les ressources syst√®me (utilisation du CPU, m√©moire disponible, etc.). Ces informations sont utilis√©es pour prendre des d√©cisions concernant l'algorithme de compression et ses param√®tres.
Interaction avec le syst√®me Hadoop
L‚Äôarchitecture SMA est int√©gr√©e de mani√®re √† ce que les agents soient capables de surveiller les fichiers dans HDFS, d'analyser les t√¢ches de MapReduce, et de modifier en temps r√©el les algorithmes de compression. Voici les principales interactions :
Moniteur de Fichiers : L'Agent de Profiling surveille en continu les fichiers entrant dans HDFS, recueillant des informations comme le type de donn√©es (texte, binaire, multim√©dia), la taille, et la fr√©quence d'acc√®s.
Interaction avec MapReduce : Pendant le traitement des t√¢ches MapReduce, les agents surveillent l'utilisation des ressources (CPU, m√©moire, etc.) et ajustent les algorithmes en cons√©quence. Cela permet de r√©duire les goulots d'√©tranglement lors des transferts de donn√©es inter